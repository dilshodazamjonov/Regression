{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdc7386",
   "metadata": {},
   "source": [
    "## Weed Detection learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95aac7a",
   "metadata": {},
   "source": [
    "## Converting the dataset from the XML to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909f36cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Converted 4223 images and labels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import shutil\n",
    "\n",
    "# --- Paths ---\n",
    "src_dirs = [\n",
    "    r\"D:\\python projects\\MachineLearning\\WeedDetection\\data\\Year2021_Part1\",\n",
    "    r\"D:\\python projects\\MachineLearning\\WeedDetection\\data\\Year2021_Part2\",\n",
    "    r\"D:\\python projects\\MachineLearning\\WeedDetection\\data\\Year2022\"\n",
    "]\n",
    "output_dir = Path(\"D:\\\\python projects\\\\MachineLearning\\\\WeedDetection\\\\data\\\\yolo_dataset\")\n",
    "images_out = output_dir / \"images\"\n",
    "labels_out = output_dir / \"labels\"\n",
    "images_out.mkdir(parents=True, exist_ok=True)\n",
    "labels_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLASS_NAMES = [\"Waterhemp\",\"Carpetweed\",\"Morninglory\",\"Goosegrass\",\"Spotted Spurge\",\"Palmer Amaranth\",\"Purslane\",\"Ragweed\"]\n",
    "CLASS_MAP = {c.lower(): i for i, c in enumerate(CLASS_NAMES)}\n",
    "\n",
    "def xml_to_yolo(xml_file, img_w, img_h):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    yolo_obj = []\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        cls_name = obj.findtext('name').lower()\n",
    "        cls_idx = CLASS_MAP.get(cls_name)\n",
    "        \n",
    "        if cls_idx is None:\n",
    "            continue\n",
    "\n",
    "        bnd = obj.find('bndbox')\n",
    "        xmin = int(float(bnd.findtext('xmin')))\n",
    "        ymin = int(float(bnd.findtext('ymin')))\n",
    "        xmax = int(float(bnd.findtext('xmax')))\n",
    "        ymax = int(float(bnd.findtext('ymax')))\n",
    "        x_center = ((xmin + xmax) / 2) / img_w\n",
    "        y_center = ((ymin + ymax) / 2) / img_h\n",
    "        w = (xmax - xmin) / img_w\n",
    "        h = (ymax - ymin) / img_h\n",
    "        \n",
    "        yolo_obj.append(f\"{cls_idx} {x_center:.6f} {y_center:.6f} {w:.6f} {h:.6f}\")\n",
    "    return yolo_obj\n",
    "\n",
    "for src in src_dirs:\n",
    "    \n",
    "    src_path = Path(src)\n",
    "    \n",
    "    for img_file in src_path.glob('*.jpg'):\n",
    "    \n",
    "        base = img_file.stem\n",
    "        xml_file = src_path / f\"{base}.xml\"\n",
    "\n",
    "        if not xml_file.exists():\n",
    "            continue\n",
    "\n",
    "        with Image.open(img_file) as im:\n",
    "            w, h = im.size\n",
    "        yolo_obj = xml_to_yolo(xml_file, w, h)\n",
    "\n",
    "        if not yolo_obj:\n",
    "            continue\n",
    "    \n",
    "        label_path = labels_out / f\"{base}.txt\"\n",
    "        with open(label_path, mode='w') as f:\n",
    "            f.write('\\n'.join(yolo_obj))\n",
    "\n",
    "        shutil.copy(img_file, images_out / img_file.name)\n",
    "        \n",
    "print(f\"Done! Converted {len(list(images_out.glob('*.jpg')))} images and labels\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df983eeb",
   "metadata": {},
   "source": [
    "Now that your images and labels are ready, the next step is to create the data.yaml file, which YOLOv8 uses to know:\n",
    "<br>\n",
    "1. Where the train and validation images are <br>\n",
    "<br>\n",
    "2. Where the labels are <br>\n",
    "<br>\n",
    "3. How many classes exist <br>\n",
    "<br>\n",
    "4. What the class names are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483d3a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.yaml created at D:\\python projects\\MachineLearning\\WeedDetection\\data\\yolo_dataset\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml \n",
    "import random\n",
    "\n",
    "output_dir = Path(r\"D:\\\\python projects\\\\MachineLearning\\\\WeedDetection\\\\data\\\\yolo_dataset\")\n",
    "images_dir = output_dir / 'images'\n",
    "\n",
    "images = list(images_dir.glob('*.jpg'))\n",
    "random.shuffle(images)\n",
    "split_idx = int(len(images) * 0.8)\n",
    "\n",
    "train_imgs = images[:split_idx]\n",
    "val_imgs = images[split_idx:]\n",
    "\n",
    "\n",
    "# ======= Folders for Training and Validation sets of images =======\n",
    "\n",
    "train_dir = output_dir / \"train/images\"\n",
    "val_dir = output_dir / \"val/images\"\n",
    "train_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ======= Cp of images and labels =======\n",
    "\n",
    "train_labels_dir = output_dir / \"train/labels\"\n",
    "val_labels_dir = output_dir / \"val/labels\"\n",
    "train_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for img in train_imgs:\n",
    "    shutil.copy2(img, train_dir / img.name)\n",
    "    shutil.copy2(output_dir / \"labels\" / f\"{img.stem}.txt\", train_labels_dir / f\"{img.stem}.txt\")\n",
    "\n",
    "for img in val_imgs:\n",
    "    shutil.copy2(img, val_dir / img.name)\n",
    "    shutil.copy2(output_dir / \"labels\" / f\"{img.stem}.txt\", val_labels_dir / f\"{img.stem}.txt\")\n",
    "\n",
    "data_yaml_path = output_dir / \"data.yaml\"\n",
    "\n",
    "data_dict = {\n",
    "    \"train\": str(train_dir),\n",
    "    \"val\": str(val_dir),\n",
    "    \"nc\": len(CLASS_NAMES),\n",
    "    \"names\": CLASS_NAMES\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open(data_yaml_path, mode='w') as f:\n",
    "        yaml.dump(data_dict, f)\n",
    "except Exception as e:\n",
    "    print(f\"Error occured while saving the Yaml file: {e}\")\n",
    "\n",
    "print(f\"data.yaml created at {data_yaml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d1a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3378"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(val_imgs))\n",
    "len(train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96d82a",
   "metadata": {},
   "source": [
    "## Training with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33a8026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.228  Python-3.12.6 torch-2.9.1+cpu CPU (13th Gen Intel Core i7-13620H)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=D:\\python projects\\MachineLearning\\WeedDetection\\data\\yolo_dataset\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=weed_detection2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=D:\\python projects\\MachineLearning\\WeedDetection\\runs\\weed_detection2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=8\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 2321.8534.3 MB/s, size: 5457.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\python projects\\MachineLearning\\WeedDetection\\data\\yolo_dataset\\train\\labels.cache... 3378 images, 1 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 3379/3379  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 2739.9201.6 MB/s, size: 5119.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\python projects\\MachineLearning\\WeedDetection\\data\\yolo_dataset\\val\\labels.cache... 845 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 845/845 1.6Mit/s 0.0s\n",
      "Plotting labels to D:\\python projects\\MachineLearning\\WeedDetection\\runs\\weed_detection2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mD:\\python projects\\MachineLearning\\WeedDetection\\runs\\weed_detection2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50         0G     0.9627      2.462      1.263          7        640: 100% ━━━━━━━━━━━━ 423/423 2.2s/it 15:48<1.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 2.4s/it 2:072.7ss\n",
      "                   all        845       1532      0.604      0.636      0.626      0.503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/50         0G     0.8474      1.606      1.171         11        640: 100% ━━━━━━━━━━━━ 423/423 2.2s/it 15:48<1.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 2.4s/it 2:072.5ss\n",
      "                   all        845       1532      0.727      0.685      0.738      0.592\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/50         0G     0.8504      1.425      1.177         10        640: 100% ━━━━━━━━━━━━ 423/423 2.0s/it 14:08<1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 2.0s/it 1:462.3sss\n",
      "                   all        845       1532      0.616      0.712      0.738      0.579\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/50         0G     0.8171      1.242      1.142          9        640: 100% ━━━━━━━━━━━━ 423/423 1.9s/it 13:43<1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 1.7s/it 1:311.9sss\n",
      "                   all        845       1532      0.798      0.705      0.785      0.639\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/50         0G     0.7776       1.11       1.13         11        640: 100% ━━━━━━━━━━━━ 423/423 1.8s/it 12:25<1.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 2.2s/it 1:591.9sss\n",
      "                   all        845       1532      0.837      0.751       0.85      0.705\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/50         0G     0.7555      1.023      1.121          4        640: 100% ━━━━━━━━━━━━ 423/423 2.1s/it 14:47<1.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 2.4s/it 2:062.6ss\n",
      "                   all        845       1532      0.854      0.777      0.863      0.721\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/50         0G     0.7361     0.9556      1.106         11        640: 100% ━━━━━━━━━━━━ 423/423 2.1s/it 14:52<1.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 2.3s/it 2:032.5sss\n",
      "                   all        845       1532      0.869      0.756      0.862      0.727\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/50         0G     0.7345     0.9673      1.108         15        640: 100% ━━━━━━━━━━━━ 423/423 2.1s/it 14:49<1.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 1.9s/it 1:412.4sss\n",
      "                   all        845       1532      0.842      0.777      0.863      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/50         0G     0.7102     0.8988      1.093          4        640: 100% ━━━━━━━━━━━━ 423/423 2.0s/it 13:48<1.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 1.8s/it 1:341.8sss\n",
      "                   all        845       1532      0.868      0.807      0.892      0.768\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/50         0G     0.7022     0.8519       1.09         10        640: 100% ━━━━━━━━━━━━ 423/423 1.9s/it 13:28<1.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 2.3s/it 2:022.5sss\n",
      "                   all        845       1532       0.86      0.811      0.887      0.765\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/50         0G     0.6823     0.8466      1.084         11        640: 100% ━━━━━━━━━━━━ 423/423 2.0s/it 13:58<1.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 53/53 1.7s/it 1:321.9sss\n",
      "                   all        845       1532      0.838      0.782      0.859      0.732\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/50         0G       0.68     0.8123      1.082         27        640: 68% ━━━━━━━━──── 286/423 1.9it/s 10:25<1:12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:860\u001b[39m, in \u001b[36mPath.exists\u001b[39m\u001b[34m(self, follow_symlinks)\u001b[39m\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:840\u001b[39m, in \u001b[36mPath.stat\u001b[39m\u001b[34m(self, follow_symlinks)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[33;03mReturn the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[32m    838\u001b[39m \u001b[33;03mos.stat() does.\u001b[39;00m\n\u001b[32m    839\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 2] The system cannot find the file specified: 'D:\\\\python projects\\\\MachineLearning\\\\WeedDetection\\\\data\\\\yolo_dataset\\\\train\\\\images\\\\20210912_CanonEOS4000D_YL_61.npy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m      3\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolov8n.pt\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_yaml_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# number of CPU threads, can increase to 12-14\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweed_detection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:778\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    775\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    776\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:243\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:404\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    402\u001b[39m     pbar = TQDM(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_loader), total=nb)\n\u001b[32m    403\u001b[39m \u001b[38;5;28mself\u001b[39m.tloss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_batch_start\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Warmup\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\ultralytics\\utils\\tqdm.py:351\u001b[39m, in \u001b[36mTQDM.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mNoneType\u001b[39m\u001b[33m'\u001b[39m\u001b[33m object is not iterable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\ultralytics\\data\\build.py:76\u001b[39m, in \u001b[36mInfiniteDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\ultralytics\\data\\base.py:374\u001b[39m, in \u001b[36mBaseDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    373\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return transformed label information for given index.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_image_and_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\ultralytics\\data\\base.py:387\u001b[39m, in \u001b[36mBaseDataset.get_image_and_label\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    385\u001b[39m label = deepcopy(\u001b[38;5;28mself\u001b[39m.labels[index])  \u001b[38;5;66;03m# requires deepcopy() https://github.com/ultralytics/ultralytics/pull/1948\u001b[39;00m\n\u001b[32m    386\u001b[39m label.pop(\u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# shape is for rect, remove it\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m label[\u001b[33m\"\u001b[39m\u001b[33mimg\u001b[39m\u001b[33m\"\u001b[39m], label[\u001b[33m\"\u001b[39m\u001b[33mori_shape\u001b[39m\u001b[33m\"\u001b[39m], label[\u001b[33m\"\u001b[39m\u001b[33mresized_shape\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m label[\u001b[33m\"\u001b[39m\u001b[33mratio_pad\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    389\u001b[39m     label[\u001b[33m\"\u001b[39m\u001b[33mresized_shape\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m] / label[\u001b[33m\"\u001b[39m\u001b[33mori_shape\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    390\u001b[39m     label[\u001b[33m\"\u001b[39m\u001b[33mresized_shape\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m1\u001b[39m] / label[\u001b[33m\"\u001b[39m\u001b[33mori_shape\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m1\u001b[39m],\n\u001b[32m    391\u001b[39m )  \u001b[38;5;66;03m# for evaluation\u001b[39;00m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rect:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\python projects\\MachineLearning\\venv\\Lib\\site-packages\\ultralytics\\data\\base.py:225\u001b[39m, in \u001b[36mBaseDataset.load_image\u001b[39m\u001b[34m(self, i, rect_mode)\u001b[39m\n\u001b[32m    223\u001b[39m im, f, fn = \u001b[38;5;28mself\u001b[39m.ims[i], \u001b[38;5;28mself\u001b[39m.im_files[i], \u001b[38;5;28mself\u001b[39m.npy_files[i]\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# not cached in RAM\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# load npy\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    227\u001b[39m             im = np.load(fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:862\u001b[39m, in \u001b[36mPath.exists\u001b[39m\u001b[34m(self, follow_symlinks)\u001b[39m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28mself\u001b[39m.stat(follow_symlinks=follow_symlinks)\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_ignore_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    863\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    864\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:52\u001b[39m, in \u001b[36m_ignore_error\u001b[39m\u001b[34m(exception)\u001b[39m\n\u001b[32m     45\u001b[39m _IGNORED_ERRNOS = (ENOENT, ENOTDIR, EBADF, ELOOP)\n\u001b[32m     47\u001b[39m _IGNORED_WINERRORS = (\n\u001b[32m     48\u001b[39m     _WINERROR_NOT_READY,\n\u001b[32m     49\u001b[39m     _WINERROR_INVALID_NAME,\n\u001b[32m     50\u001b[39m     _WINERROR_CANT_RESOLVE_FILENAME)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ignore_error\u001b[39m(exception):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mgetattr\u001b[39m(exception, \u001b[33m'\u001b[39m\u001b[33merrno\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m _IGNORED_ERRNOS \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m     54\u001b[39m             \u001b[38;5;28mgetattr\u001b[39m(exception, \u001b[33m'\u001b[39m\u001b[33mwinerror\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m _IGNORED_WINERRORS)\n\u001b[32m     57\u001b[39m \u001b[38;5;129m@functools\u001b[39m.cache\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_case_sensitive\u001b[39m(flavour):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO # type: ignore\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\") \n",
    "\n",
    "results = model.train(\n",
    "    data=str(data_yaml_path),\n",
    "    epochs=50,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    workers=8,       # number of CPU threads, can increase to 12-14\n",
    "    name=\"weed_detection\",\n",
    "    project=\"runs\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729bb91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
